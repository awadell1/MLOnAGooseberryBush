{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.9 64-bit (conda)",
      "metadata": {
        "interpreter": {
          "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
        }
      }
    },
    "colab": {
      "name": "SampleNotebook.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAmmyLdvzd0"
      },
      "source": [
        "# Load Dataset\n",
        "import pandas\n",
        "import re\n",
        "\n",
        "# Load Kaggle Wine Dataset: https://www.kaggle.com/zynicide/wine-reviews\n",
        "try:\n",
        "  wine_reviews = pandas.read_csv(\"../data/winemag-data-130k-v2.csv\")\n",
        "except:\n",
        "  wine_reviews = pandas.read_csv(\"https://drive.google.com/uc?export=download&id=1UFKyzq8aTg-1hgYVxVA0bm7D2mmKqSk9\")\n",
        "\n",
        "# Parse Year from Title\n",
        "year = []\n",
        "for title in wine_reviews.title:\n",
        "    year_match = re.search(\"(\\d{4})\", title)\n",
        "    if year_match:\n",
        "        year.append(year_match.group(1))\n",
        "    else:\n",
        "        year.append(None)\n",
        "\n",
        "wine_reviews.insert(wine_reviews.shape[1], value=year, column=\"year\")\n",
        "\n",
        "# Drop incomplete data\n",
        "wine_reviews.drop(columns=\"Unnamed: 0\", inplace=True)\n",
        "wine_reviews.dropna(axis=0, inplace=True)\n",
        "\n",
        "# Correct Data Types\n",
        "wine_reviews = wine_reviews.astype({\n",
        "    \"country\": \"category\",\n",
        "    \"description\": \"string\",\n",
        "    \"designation\": \"category\",\n",
        "    \"points\": \"int64\",\n",
        "    \"price\": \"float64\",\n",
        "    \"province\": \"category\",\n",
        "    \"region_1\": \"category\",\n",
        "    \"region_2\": \"category\",\n",
        "    \"taster_name\": \"category\",\n",
        "    \"taster_twitter_handle\": \"category\",\n",
        "    \"title\": \"string\",\n",
        "    \"variety\": \"category\",\n",
        "    \"winery\": \"category\",\n",
        "    \"year\": \"int64\",\n",
        "})"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXRHkvsEZCa9"
      },
      "source": [
        "# Transfom and Featurize Data\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.preprocessing import *\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build list of column transformers\n",
        "col_tf = {}\n",
        "\n",
        "# Drop Points\n",
        "col_tf[\"standard\"] = [\n",
        "    (\"points\", \"drop\", make_column_selector(\"points\")),\n",
        "    (\"price\", PowerTransformer(method=\"box-cox\"), make_column_selector(\"price\")),\n",
        "    (\"year\", MinMaxScaler(), make_column_selector(\"year\")),\n",
        "    (\"taster\", \"drop\", make_column_selector(\"taster_*\")),\n",
        "]\n",
        "\n",
        "# Build OneHot and Ordinal Encoders for Catergorial Data\n",
        "col_tf[\"OneHotEncoder\"] = []\n",
        "col_tf[\"OrdinalEncoder\"] = []\n",
        "for feat in wine_reviews.select_dtypes(include=['category']).columns:\n",
        "    categories = wine_reviews[feat].unique()\n",
        "    onehot = OneHotEncoder(categories=[categories])\n",
        "    ordinal = OrdinalEncoder(categories=[categories])\n",
        "    col_tf[\"OneHotEncoder\"].append((feat, onehot, make_column_selector(feat)))\n",
        "    col_tf[\"OrdinalEncoder\"].append((feat, ordinal, make_column_selector(feat)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYKsVzR5lO7f"
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Sparse to Dense Transformer\n",
        "DenseTransformer = FunctionTransformer(\n",
        "    func = lambda x: x.toarray(),\n",
        "    accept_sparse=True,\n",
        ")\n",
        "\n",
        "# Requires Dense\n",
        "req_dense = [\n",
        "    \"GaussianProcessClassifier\",\n",
        "    \"GaussianProcessRegressor\",\n",
        "    \"QuadraticDiscriminantAnalysis\",\n",
        "]\n",
        "\n",
        "def add_dense_tf(steps):\n",
        "    if steps[-1][0] in req_dense:\n",
        "        steps.insert(-1, (\"DenseTransformer\", DenseTransformer))\n",
        "    return steps"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8WKuYEKW5ZC"
      },
      "source": [
        "# Preallocate a list of models to train\n",
        "models = []"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptJpqwmou-MY"
      },
      "source": [
        "# Try Various Classifers\n",
        "import re\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# One-Hot + Continous Data\n",
        "classifers = [\n",
        "    KNeighborsClassifier(),\n",
        "    LinearSVC(),\n",
        "    SVC(kernel=\"rbf\"),\n",
        "    SVC(kernel=\"poly\"),\n",
        "    SVC(kernel=\"sigmoid\"),\n",
        "    #GaussianProcessClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    #QuadraticDiscriminantAnalysis(),\n",
        "    MLPClassifier(hidden_layer_sizes=(10000, 100,), activation=\"relu\"),\n",
        "]\n",
        "\n",
        "for c in classifers:\n",
        "    steps = [\n",
        "        (\"Std&OneHot\", ColumnTransformer(col_tf[\"standard\"]+col_tf[\"OneHotEncoder\"])),\n",
        "        (type(c).__name__, c),\n",
        "    ]\n",
        "    steps = add_dense_tf(steps)\n",
        "    p = Pipeline(steps, verbose=True)\n",
        "    models.append(p)\n",
        "\n",
        "steps[0][1].fit_transform(wine_reviews).shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22047, 12451)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpvklyHpaFVR"
      },
      "source": [
        "# Ordinal Classifers\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "\n",
        "# Build Pipeline - Catergorical Only\n",
        "steps = [\n",
        "    (\"OrdinalCatOnly\", ColumnTransformer(col_tf[\"OrdinalEncoder\"])),\n",
        "    (\"CateCategoricalNB\", CategoricalNB()),\n",
        "]\n",
        "steps = add_dense_tf(steps)\n",
        "p = Pipeline(steps, verbose=True)\n",
        "models.append(p)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6qUWh7MgecH"
      },
      "source": [
        "# Regression Models\n",
        "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "regressors = [\n",
        "    AdaBoostRegressor(),\n",
        "    RandomForestRegressor(),\n",
        "    #GaussianProcessRegressor(),\n",
        "    LinearRegression(),\n",
        "    Ridge(),\n",
        "    ElasticNet(),\n",
        "    Lasso(),\n",
        "    KNeighborsRegressor(),\n",
        "    MLPRegressor(),\n",
        "    LinearSVC(),\n",
        "    DecisionTreeRegressor(),\n",
        "]\n",
        "\n",
        "for r in regressors:\n",
        "    steps = [\n",
        "        (\"Std&OneHot\", ColumnTransformer(col_tf[\"standard\"]+col_tf[\"OneHotEncoder\"])),\n",
        "        (type(r).__name__, r),\n",
        "    ]\n",
        "    steps = add_dense_tf(steps)\n",
        "    p = Pipeline(steps, verbose=True)\n",
        "    models.append(p)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNTjxzrNmpv4"
      },
      "source": [
        "# Build Label Pipelines\n",
        "label_pipe = {}\n",
        "\n",
        "def point_pipe(name, tf):\n",
        "    return ColumnTransformer([(name, tf, make_column_selector(\"points\"))])\n",
        "\n",
        "# Bin all scores\n",
        "label_pipe[\"Classifer\"] = point_pipe(\"BinScores\",\n",
        "    KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        ")\n",
        "\n",
        "# Normalize Scores to a Gaussian\n",
        "label_pipe[\"Regression\"] = point_pipe(\"NormScores\", StandardScaler())\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCggz5xuNd3g"
      },
      "source": [
        "# Enable Verbose and Parallel Models\n",
        "for m in models:\n",
        "    for k in m.get_params(deep=True).keys():\n",
        "        new_param = {}\n",
        "        if re.match(\".*n_job\", k):\n",
        "            new_param[k] = -1\n",
        "        if re.match(\".*verbose\", k):\n",
        "            new_param[k] = True\n",
        "        if new_param:\n",
        "            m.set_params(**new_param)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAsZXwg6oejr"
      },
      "source": [
        "Models to Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiqlJE_7ohHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21728fb-5d72-4010-ebb8-b8980c580f57"
      },
      "source": [
        "for idx, m in enumerate(models):\n",
        "    label = \" -> \".join([s[0] for s in m.steps])\n",
        "    print(f\"{idx:3d}: {label}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0: Std&OneHot -> KNeighborsClassifier\n  1: Std&OneHot -> LinearSVC\n  2: Std&OneHot -> SVC\n  3: Std&OneHot -> SVC\n  4: Std&OneHot -> SVC\n  5: Std&OneHot -> AdaBoostClassifier\n  6: Std&OneHot -> DecisionTreeClassifier\n  7: Std&OneHot -> RandomForestClassifier\n  8: Std&OneHot -> MLPClassifier\n  9: OrdinalCatOnly -> CateCategoricalNB\n 10: Std&OneHot -> AdaBoostRegressor\n 11: Std&OneHot -> RandomForestRegressor\n 12: Std&OneHot -> LinearRegression\n 13: Std&OneHot -> Ridge\n 14: Std&OneHot -> ElasticNet\n 15: Std&OneHot -> Lasso\n 16: Std&OneHot -> KNeighborsRegressor\n 17: Std&OneHot -> MLPRegressor\n 18: Std&OneHot -> LinearSVC\n 19: Std&OneHot -> DecisionTreeRegressor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZETnSCFFcuC7"
      },
      "source": [
        "Train Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6U7tF4Ucl2E",
        "outputId": "e73acf2d-26a6-4e00-d81a-0e59d3c12524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.base import is_classifier\n",
        "\n",
        "# Split into Test/Train Sets\n",
        "train, test = train_test_split(wine_reviews, test_size=0.2, shuffle=True)\n",
        "\n",
        "for model in models:\n",
        "    # Transform Labels based on Model Type\n",
        "    if is_classifier(model):\n",
        "        y_pipe = label_pipe[\"Classifer\"]\n",
        "    else:\n",
        "        y_pipe = label_pipe[\"Regression\"]\n",
        "\n",
        "    trainY = y_pipe.fit_transform(train).ravel()\n",
        "    testY =  y_pipe.transform(test).ravel()\n",
        "\n",
        "    print(\"\\nTraining: %s\" % model.steps[-1][0])\n",
        "\n",
        "    try:\n",
        "        model.fit(train, trainY)\n",
        "        print(f\"Train: {model.score(train, trainY)}\")\n",
        "        print(f\"Test: {model.score(test, testY)}\")\n",
        "    except:\n",
        "        print(f\"Error Training Model\")\n",
        "        print(sys.exc_info())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training: KNeighborsClassifier\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   2.8s\n",
            "[Pipeline]  (step 2 of 2) Processing KNeighborsClassifier, total=   0.0s\n",
            "Train: 0.8459488575154505\n",
            "Test: 0.7902494331065759\n",
            "\n",
            "Training: LinearSVC\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[LibLinear][Pipeline] ......... (step 2 of 2) Processing LinearSVC, total=   0.7s\n",
            "Train: 0.9263480183704712\n",
            "Test: 0.8058956916099773\n",
            "\n",
            "Training: SVC\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[LibSVM][Pipeline] ............... (step 2 of 2) Processing SVC, total=  20.3s\n",
            "Train: 0.8267279015705619\n",
            "Test: 0.8081632653061225\n",
            "\n",
            "Training: SVC\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[LibSVM][Pipeline] ............... (step 2 of 2) Processing SVC, total=  22.2s\n",
            "Train: 0.8562680728014969\n",
            "Test: 0.8163265306122449\n",
            "\n",
            "Training: SVC\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[LibSVM][Pipeline] ............... (step 2 of 2) Processing SVC, total=   7.0s\n",
            "Train: 0.7346487497873788\n",
            "Test: 0.7222222222222222\n",
            "\n",
            "Training: AdaBoostClassifier\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline]  (step 2 of 2) Processing AdaBoostClassifier, total=   8.4s\n",
            "Train: 0.7847706526053184\n",
            "Test: 0.7832199546485261\n",
            "\n",
            "Training: DecisionTreeClassifier\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline]  (step 2 of 2) Processing DecisionTreeClassifier, total=   2.2s\n",
            "Train: 0.9998299030447355\n",
            "Test: 0.7918367346938775\n",
            "\n",
            "Training: RandomForestClassifier\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.2s finished\n",
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Pipeline]  (step 2 of 2) Processing RandomForestClassifier, total=   6.4s\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "Train: 0.9998299030447355\n",
            "Test: 0.81859410430839\n",
            "\n",
            "Training: MLPClassifier\n",
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "Iteration 1, loss = 0.52413175\n",
            "Iteration 2, loss = 0.34014096\n",
            "Iteration 3, loss = 0.23409335\n",
            "Iteration 4, loss = 0.18435688\n",
            "Iteration 5, loss = 0.15743695\n",
            "Iteration 6, loss = 0.13960444\n",
            "Iteration 7, loss = 0.12617319\n",
            "Iteration 8, loss = 0.11872661\n",
            "Iteration 9, loss = 0.11262694\n",
            "Iteration 10, loss = 0.10694751\n",
            "Iteration 11, loss = 0.10257410\n",
            "Iteration 12, loss = 0.09915043\n",
            "Iteration 13, loss = 0.09770520\n",
            "Iteration 14, loss = 0.09502261\n",
            "Iteration 15, loss = 0.09137107\n",
            "Iteration 16, loss = 0.09178979\n",
            "Iteration 17, loss = 0.08971919\n",
            "Iteration 18, loss = 0.08797664\n",
            "Iteration 19, loss = 0.08648602\n",
            "Iteration 20, loss = 0.08560207\n",
            "Iteration 21, loss = 0.08412803\n",
            "Iteration 22, loss = 0.08194587\n",
            "Iteration 23, loss = 0.08275229\n",
            "Iteration 24, loss = 0.08054035\n",
            "Iteration 25, loss = 0.08052174\n",
            "Iteration 26, loss = 0.08379386\n",
            "Iteration 27, loss = 0.08473913\n",
            "Iteration 28, loss = 0.08499027\n",
            "Iteration 29, loss = 0.08969188\n",
            "Iteration 30, loss = 0.08758830\n",
            "Iteration 31, loss = 0.08321208\n",
            "Iteration 32, loss = 0.08563063\n",
            "Iteration 33, loss = 0.08102723\n",
            "Iteration 34, loss = 0.07999912\n",
            "Iteration 35, loss = 0.07843853\n",
            "Iteration 36, loss = 0.07676191\n",
            "Iteration 37, loss = 0.07672278\n",
            "Iteration 38, loss = 0.07530534\n",
            "Iteration 39, loss = 0.07340847\n",
            "Iteration 40, loss = 0.07778884\n",
            "Iteration 41, loss = 0.08149298\n",
            "Iteration 42, loss = 0.08211279\n",
            "Iteration 43, loss = 0.07629287\n",
            "Iteration 44, loss = 0.07339035\n",
            "Iteration 45, loss = 0.07522875\n",
            "Iteration 46, loss = 0.07349342\n",
            "Iteration 47, loss = 0.07267393\n",
            "Iteration 48, loss = 0.07611770\n",
            "Iteration 49, loss = 0.07707066\n",
            "Iteration 50, loss = 0.08152652\n",
            "Iteration 51, loss = 0.07685322\n",
            "Iteration 52, loss = 0.07565051\n",
            "Iteration 53, loss = 0.07534249\n",
            "Iteration 54, loss = 0.07278100\n",
            "Iteration 55, loss = 0.07376408\n",
            "Iteration 56, loss = 0.07715204\n",
            "Iteration 57, loss = 0.07568393\n",
            "Iteration 58, loss = 0.07593086\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[Pipeline] .... (step 2 of 2) Processing MLPClassifier, total=375.9min\n",
            "Train: 0.9627487667970743\n",
            "Test: 0.7845804988662132\n",
            "\n",
            "Training: CateCategoricalNB\n",
            "[Pipeline] .... (step 1 of 2) Processing OrdinalCatOnly, total=   0.1s\n",
            "[Pipeline] . (step 2 of 2) Processing CateCategoricalNB, total=   0.0s\n",
            "Train: 0.8338152747065828\n",
            "Error Training Model\n",
            "(<class 'IndexError'>, IndexError('index 168 is out of bounds for axis 1 with size 168'), <traceback object at 0x000001A0DB7AE508>)\n",
            "\n",
            "Training: AdaBoostRegressor\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline] . (step 2 of 2) Processing AdaBoostRegressor, total=   1.5s\n",
            "Train: 0.2791981422001436\n",
            "Test: 0.27024031385157554\n",
            "\n",
            "Training: RandomForestRegressor\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   13.5s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   40.9s finished\n",
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Pipeline]  (step 2 of 2) Processing RandomForestRegressor, total=  41.1s\n",
            "Train: 0.9284216717232162\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "Test: 0.4797756915723784\n",
            "\n",
            "Training: LinearRegression\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline] .. (step 2 of 2) Processing LinearRegression, total=   3.9s\n",
            "Train: 0.8462576817819385\n",
            "Test: 0.2890944130600115\n",
            "\n",
            "Training: Ridge\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline] ............. (step 2 of 2) Processing Ridge, total=   0.3s\n",
            "Train: 0.7812477998696715\n",
            "Test: 0.49232867844304484\n",
            "\n",
            "Training: ElasticNet\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing ElasticNet, total=   0.6s\n",
            "Train: 0.009250875599922215\n",
            "Test: 0.00874705702667855\n",
            "\n",
            "Training: Lasso\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline] ............. (step 2 of 2) Processing Lasso, total=   0.4s\n",
            "Train: 0.0\n",
            "Test: -0.0002596174005269347\n",
            "\n",
            "Training: KNeighborsRegressor\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline]  (step 2 of 2) Processing KNeighborsRegressor, total=   0.0s\n",
            "Train: 0.6246510699741774\n",
            "Test: 0.42147943606024607\n",
            "\n",
            "Training: MLPRegressor\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "Iteration 1, loss = 0.32872489\n",
            "Iteration 2, loss = 0.21727614\n",
            "Iteration 3, loss = 0.16355694\n",
            "Iteration 4, loss = 0.13338488\n",
            "Iteration 5, loss = 0.11460436\n",
            "Iteration 6, loss = 0.10319369\n",
            "Iteration 7, loss = 0.09609234\n",
            "Iteration 8, loss = 0.09058570\n",
            "Iteration 9, loss = 0.08504273\n",
            "Iteration 10, loss = 0.08199838\n",
            "Iteration 11, loss = 0.07845143\n",
            "Iteration 12, loss = 0.07634252\n",
            "Iteration 13, loss = 0.07254813\n",
            "Iteration 14, loss = 0.07083316\n",
            "Iteration 15, loss = 0.06856134\n",
            "Iteration 16, loss = 0.06630233\n",
            "Iteration 17, loss = 0.06546843\n",
            "Iteration 18, loss = 0.06421861\n",
            "Iteration 19, loss = 0.06191868\n",
            "Iteration 20, loss = 0.06167540\n",
            "Iteration 21, loss = 0.05926744\n",
            "Iteration 22, loss = 0.05817975\n",
            "Iteration 23, loss = 0.05729138\n",
            "Iteration 24, loss = 0.05654588\n",
            "Iteration 25, loss = 0.05613811\n",
            "Iteration 26, loss = 0.05526103\n",
            "Iteration 27, loss = 0.05459797\n",
            "Iteration 28, loss = 0.05369959\n",
            "Iteration 29, loss = 0.05328065\n",
            "Iteration 30, loss = 0.05318368\n",
            "Iteration 31, loss = 0.05266842\n",
            "Iteration 32, loss = 0.05181063\n",
            "Iteration 33, loss = 0.05123013\n",
            "Iteration 34, loss = 0.05076324\n",
            "Iteration 35, loss = 0.05076366\n",
            "Iteration 36, loss = 0.05019941\n",
            "Iteration 37, loss = 0.04991865\n",
            "Iteration 38, loss = 0.04959432\n",
            "Iteration 39, loss = 0.04921415\n",
            "Iteration 40, loss = 0.04902903\n",
            "Iteration 41, loss = 0.04848120\n",
            "Iteration 42, loss = 0.04857941\n",
            "Iteration 43, loss = 0.04833799\n",
            "Iteration 44, loss = 0.04814323\n",
            "Iteration 45, loss = 0.04756875\n",
            "Iteration 46, loss = 0.04728582\n",
            "Iteration 47, loss = 0.04746585\n",
            "Iteration 48, loss = 0.04700463\n",
            "Iteration 49, loss = 0.04680558\n",
            "Iteration 50, loss = 0.04654655\n",
            "Iteration 51, loss = 0.04649951\n",
            "Iteration 52, loss = 0.04627062\n",
            "Iteration 53, loss = 0.04605824\n",
            "Iteration 54, loss = 0.04587593\n",
            "Iteration 55, loss = 0.04573130\n",
            "Iteration 56, loss = 0.04556412\n",
            "Iteration 57, loss = 0.04587371\n",
            "Iteration 58, loss = 0.04527653\n",
            "Iteration 59, loss = 0.04533218\n",
            "Iteration 60, loss = 0.04533654\n",
            "Iteration 61, loss = 0.04501710\n",
            "Iteration 62, loss = 0.04485698\n",
            "Iteration 63, loss = 0.04476415\n",
            "Iteration 64, loss = 0.04437729\n",
            "Iteration 65, loss = 0.04455402\n",
            "Iteration 66, loss = 0.04437153\n",
            "Iteration 67, loss = 0.04445400\n",
            "Iteration 68, loss = 0.04437976\n",
            "Iteration 69, loss = 0.04436936\n",
            "Iteration 70, loss = 0.04405323\n",
            "Iteration 71, loss = 0.04382339\n",
            "Iteration 72, loss = 0.04358016\n",
            "Iteration 73, loss = 0.04363028\n",
            "Iteration 74, loss = 0.04347098\n",
            "Iteration 75, loss = 0.04356766\n",
            "Iteration 76, loss = 0.04330221\n",
            "Iteration 77, loss = 0.04364213\n",
            "Iteration 78, loss = 0.04347294\n",
            "Iteration 79, loss = 0.04339999\n",
            "Iteration 80, loss = 0.04335859\n",
            "Iteration 81, loss = 0.04308397\n",
            "Iteration 82, loss = 0.04326125\n",
            "Iteration 83, loss = 0.04293595\n",
            "Iteration 84, loss = 0.04288543\n",
            "Iteration 85, loss = 0.04264351\n",
            "Iteration 86, loss = 0.04291206\n",
            "Iteration 87, loss = 0.04311642\n",
            "Iteration 88, loss = 0.04300639\n",
            "Iteration 89, loss = 0.04274864\n",
            "Iteration 90, loss = 0.04243970\n",
            "Iteration 91, loss = 0.04237520\n",
            "Iteration 92, loss = 0.04233863\n",
            "Iteration 93, loss = 0.04223059\n",
            "Iteration 94, loss = 0.04226787\n",
            "Iteration 95, loss = 0.04231014\n",
            "Iteration 96, loss = 0.04245280\n",
            "Iteration 97, loss = 0.04237071\n",
            "Iteration 98, loss = 0.04232097\n",
            "Iteration 99, loss = 0.04235830\n",
            "Iteration 100, loss = 0.04160083\n",
            "Iteration 101, loss = 0.04182831\n",
            "Iteration 102, loss = 0.04185786\n",
            "Iteration 103, loss = 0.04155707\n",
            "Iteration 104, loss = 0.04175962\n",
            "Iteration 105, loss = 0.04187433\n",
            "Iteration 106, loss = 0.04154307\n",
            "Iteration 107, loss = 0.04155355\n",
            "Iteration 108, loss = 0.04146039\n",
            "Iteration 109, loss = 0.04155351\n",
            "Iteration 110, loss = 0.04126168\n",
            "Iteration 111, loss = 0.04121500\n",
            "Iteration 112, loss = 0.04133603\n",
            "Iteration 113, loss = 0.04122849\n",
            "Iteration 114, loss = 0.04132298\n",
            "Iteration 115, loss = 0.04143569\n",
            "Iteration 116, loss = 0.04126758\n",
            "Iteration 117, loss = 0.04118430\n",
            "Iteration 118, loss = 0.04113152\n",
            "Iteration 119, loss = 0.04113442\n",
            "Iteration 120, loss = 0.04105678\n",
            "Iteration 121, loss = 0.04098702\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[Pipeline] ...... (step 2 of 2) Processing MLPRegressor, total= 8.5min\n",
            "Train: 0.9252798673461233\n",
            "Test: 0.4168032373332151\n",
            "\n",
            "Training: LinearSVC\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[LibLinear][Pipeline] ......... (step 2 of 2) Processing LinearSVC, total=   0.7s\n",
            "Train: 0.9263480183704712\n",
            "Test: 0.8058956916099773\n",
            "\n",
            "Training: DecisionTreeRegressor\n",
            "[Pipeline] ........ (step 1 of 2) Processing Std&OneHot, total=   0.1s\n",
            "[Pipeline]  (step 2 of 2) Processing DecisionTreeRegressor, total=   3.7s\n",
            "Train: 0.999785443208297\n",
            "Test: 0.2467622364347164\n"
          ]
        }
      ]
    }
  ]
}